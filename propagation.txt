****** FORWARD PROPAGATION ******
WITHOUT BIAS ===
	(1) assume x is E of R^d
	(2) intermediate variable is 
		z = W(1) * x
		where W(1) : E of R^(h x d) is the weight parameter of the hidden layer.
	(3) then you run z (E of R^h) through activation function phi and you obtain 
		a hidden activation vector of length h, as follows:
		h = phi(z);
		note, h is also the hidden layer output, and is also an intermediate variable.
	(4) Now, let's assume that the parameters of the output layer only possess a weight of 
		W(2) E of R^(h x d)
		we get output layer variable as a vector of length q.
		o = W(2) x h;
	(5) We then calculate a single Loss Term (L) by inputting (o) into loss function (l) using an example label (y):
		L = l(o, y);



***** BACKWARD PROPAGATION *****
L2-norm loss:  L = 1/2 * (output - actual)^2

Sigmoid function:  S(x) = 1 / (1 + e^-x)

For a single weight U_ij  [i = previous layer, j = next layer]
partial derivative of L w.r.t. the weight that contributed to that L (so, U_ij) is:
 
 &L        &L    &y_j   &a_j(2)
-----  =  ----- ------- ------
&U_ij     &y_j  &a_j(2) &U_ij

	L = L2-norm loss
	U = weight between last hidden layer and the output layer
	a(t) = linear combination of XW + Bias before activation function is applied, v
	where t = the layer.
	y = activated linear function (i.e. a(t) with activation fx applied to it)
